ML FINAL CODES

LINEAR REGRESSION

import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd

data_set= pd.read_csv('Salary_Data.csv')

data_set.head()

x= data_set.iloc[:, :-1].values
y= data_set.iloc[:, 1].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3, random_state=42)

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(x_train, y_train)

y_pred = regressor.predict(x_test)
x_pred = regressor.predict(x_train)

mtp.scatter(x_train, y_train, color="green")
mtp.plot(x_train, x_pred, color="red")
mtp.title("Salary vs Experience (Training Dataset)")

mtp.xlabel("Years of Experience")
mtp.ylabel("Salary (In Rupees)")
mtp.show()

#test set results
mtp.plot(x_train, x_pred, color="red")
mtp.title("Salary vs Experience (Test Dataset)")
mtp.xlabel("Years of Experience")
mtp.ylabel("Salary (In Rupees)")
mtp.scatter(x_test, y_test, color="blue")
mtp.show()
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

KNN CLASSIFICATION

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# Generate synthetic dataset
X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the classifier
knn.fit(X_train, y_train)

# Define function to plot decision boundary
def plot_decision_boundary(clf, X, y, title='Decision Boundary'):
    h = .02  # step size in the mesh
    # Create color maps
    cmap_light = plt.cm.coolwarm
    cmap_bold = plt.cm.coolwarm

    # Create a mesh grid based on feature space
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Predict class using the trained classifier
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=cmap_light)

    # Plot also the training points
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=cmap_bold)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.legend(handles=scatter.legend_elements()[0], labels=['Class 0', 'Class 1'])
    plt.show()

# Plot decision boundary for the KNN classifier
plot_decision_boundary(knn, X_train, y_train, title='KNN Decision Boundary (Training Set)')
plot_decision_boundary(knn, X_test, y_test, title='KNN Decision Boundary (Test Set)')

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

KNN REGRESSION 

import numpy as nm 
import matplotlib.pyplot as mtp 
import pandas as pd

dataset= pd.read_csv('Salary_Data.csv') 

dataset.head() 

x= dataset.iloc[:, : -1].values 
y= dataset.iloc[:, 1].values 

from sklearn.model_selection import train_test_split 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3, random_state=42)

from sklearn.preprocessing import StandardScaler 
scaler = StandardScaler() 
x_train_scaled = scaler.fit_transform(x_train) 
x_test_scaled = scaler.transform(x_test)

from sklearn.neighbors import KNeighborsRegressor 
knn = KNeighborsRegressor(n_neighbors=3) 

knn.fit(x_train_scaled, y_train)
y_pred = knn.predict(x_test_scaled)

from sklearn.neighbors import KNeighborsClassifier 
classifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 ) 
classifier.fit(x_train, y_train) 

y_pred 

from sklearn.metrics import mean_squared_error, r2_score 
mse = mean_squared_error(y_test, y_pred) 
r2 = r2_score(y_test, y_pred) 
r2 

mtp.scatter(y_test, y_pred, color="green") 
mtp.xlabel('Actual Salary') 
mtp.ylabel('Predicted Salary') 
mtp.title('KNN Regression: Actual vs. Predicted Salaries') 
mtp.show() 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

LOGISTIC REGRESSION

import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd
data_set= pd.read_csv('User_Data.csv')
x= data_set.iloc[:, [2,3]].values
y= data_set.iloc[:, 4].values
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)
from sklearn.linear_model import LogisticRegression
classifier= LogisticRegression(random_state=0)
classifier.fit(x_train, y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
intercept_scaling=1, l1_ratio=None, max_iter=100,
multi_class='warn', n_jobs=None, penalty='l2',
random_state=0, solver='warn', tol=0.0001, verbose=0,
warm_start=False)
y_pred= classifier.predict(x_test)
y_pred
classifier.score(x_test, y_test)
y_pred
from matplotlib.colors import ListedColormap

# Define RGBA tuples for purple and green colors
purple = (0.5, 0, 0.5, 1)  # (R, G, B, Alpha)
green = (0, 0.5, 0, 1)      # (R, G, B, Alpha)

# Assuming you have already defined x_test, y_test, classifier, x_train, and y_train

# Use training set instead of test set for x_set, y_set
x_set, y_set = x_train, y_train

# Sample points for plotting
step = 100  # Adjust this value as needed
x1_min, x1_max = x_set[:, 0].min() - 1, x_set[:, 0].max() + 1
x2_min, x2_max = x_set[:, 1].min() - 1, x_set[:, 1].max() + 1
x1_sample = nm.linspace(x1_min, x1_max, step)
x2_sample = nm.linspace(x2_min, x2_max, step)
x1, x2 = nm.meshgrid(x1_sample, x2_sample)

# Plot decision boundary
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
             alpha=0.75, cmap=ListedColormap([purple, green]))  # Use RGBA tuples here

# Plot data points
for i, j in enumerate(nm.unique(y_set)):
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
                color=ListedColormap([purple, green])(i), label=j)  # Use RGBA tuples here

# Set plot limits
mtp.xlim(x1_min, x1_max)
mtp.ylim(x2_min, x2_max)

# Add title, labels, and legend
mtp.title('Logistic Regression (Test set)')
mtp.xlabel('Age')
mtp.ylabel('Estimated Salary')
mtp.legend()

# Show plot
mtp.show()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

SUPPORT VECTOR MACHINE 

import numpy as nm 
import matplotlib.pyplot as mtp 
import pandas as pd 
data_set= pd.read_csv('User_Data.csv') 
x= data_set.iloc[:, [2,3]].values 
y= data_set.iloc[:, 4].values
from sklearn.model_selection import train_test_split 
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=42)
from sklearn.preprocessing import StandardScaler 
st_x = StandardScaler() 
x_train = st_x.fit_transform(x_train) 
x_test = st_x.transform(x_test) 
from sklearn.svm import SVC #support vector classifier 
classifier = SVC(kernel = 'linear', random_state = 0) 
classifier.fit(x_train, y_train) 
y_pred= classifier.predict(x_test) 
from sklearn.metrics import confusion_matrix 
cm = confusion_matrix(y_test,y_pred) 
from matplotlib.colors import ListedColormap
import numpy as np
import matplotlib.pyplot as plt
x_set, y_set = x_train, y_train
x1, x2 = np.meshgrid(np.arange(start=x_set[:, 0].min() - 1, stop=x_set[:, 0].max() + 1, step=0.01),
                      np.arange(start=x_set[:, 1].min() - 1, stop=x_set[:, 1].max() + 1, step=0.01))
plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
             alpha=0.75, cmap=ListedColormap(['red', 'green']))
plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(), x2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
                color=ListedColormap(['red', 'green'])(i), label=j)
plt.title('SVM classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

RANDOM FOREST 

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np

def load_data(filename):
    """Load the dataset from a CSV file."""
    return pd.read_csv(filename)

def preprocess_data(data):
    """Preprocess the dataset."""
    X = data.iloc[:, [2, 3]].values
    y = data.iloc[:, 4].values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    return X_train, X_test, y_train, y_test

def train_model(X_train, y_train, n_estimators=100, criterion='entropy', random_state=42):
    """Train a Random Forest classifier."""
    classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, random_state=random_state)
    classifier.fit(X_train, y_train)
    return classifier

def evaluate_model(classifier, X_test, y_test):
    """Evaluate the trained model."""
    y_pred = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    print("Accuracy:", accuracy)
    print("Confusion Matrix:\n", conf_matrix)
    return accuracy, conf_matrix

def plot_decision_boundary(classifier, X_train, y_train):
    """Plot the decision boundary."""
    x_set, y_set = X_train, y_train
    x1, x2 = np.meshgrid(np.arange(start=x_set[:, 0].min() - 1, stop=x_set[:, 0].max() + 1, step=0.01),
                         np.arange(start=x_set[:, 1].min() - 1, stop=x_set[:, 1].max() + 1, step=0.01))
    plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
                 alpha=0.75, cmap=ListedColormap(['red', 'green']))
    plt.xlim(x1.min(), x1.max())
    plt.ylim(x2.min(), x2.max())
    for i, j in enumerate(np.unique(y_set)):
        plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
                    color=ListedColormap(['red', 'green'])(i), label=j)
    plt.title('Random Forest Classifier (Training set)')
    plt.xlabel('Age')
    plt.ylabel('Estimated Salary')
    plt.legend()
    plt.show()

# Load data
data_set = load_data('User_Data.csv')

# Preprocess data
X_train, X_test, y_train, y_test = preprocess_data(data_set)

# Train model
classifier = train_model(X_train, y_train)

# Evaluate model
accuracy, conf_matrix = evaluate_model(classifier, X_test, y_test)

# Plot decision boundary
plot_decision_boundary(classifier, X_train, y_train)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
